# D3 â€“Â Decentralized Distributed Data Storage Layer for Motoko

A *zeroâ€‘dependency*, **stableâ€‘heapâ€“aware** storage library that turns an InternetÂ Computer canister into a miniature object store.
It is designed for predictable performance, provable invariants, and painless testing.

---

## âœ¨Â Features

| Capability                    | Details                                                                                       |
| ----------------------------- | --------------------------------------------------------------------------------------------- |
| **Chunked file upload**       | Upload arbitrarilyâ€‘large blobs in deterministic 1.8Â MiB chunks.                               |
| **Constantâ€‘time reads**       | Direct region offsets allow `getFile`/`getFileHTTP` to stream with *O(1)* header overhead.    |
| **Spaceâ€‘efficient allocator** | Bestâ€‘fit + firstâ€‘append hybrid with coalescing free lists (offsetâ€‘ and sizeâ€‘indexed Bâ€‘Trees). |
| **Runtimeâ€‘level safety**      | 100Â % trapâ€‘onâ€‘corruption coverage via aggressive defensive checks.                            |
| **Incremental GC**            | `cleanupAbandonedUploads` walks the keyâ€‘space in bounded batches to avoid cycle overruns.     |
| **Property & scenario tests** | >2â€¯000 random operations per test run keep invariants honest.                                 |

---

## ğŸ“¦Â Installation

```bash
mops install d3
```

or add directly to your `dfx.json` / Motoko sources:

```motoko
import D3 "path/to/D3";
```

---

## ğŸš€Â Quick Start

```motoko
import D3 "mo:d3";

actor FileBucket {
  stable let d3 = D3.D3(); // 4â€¯GiB budget by default

  public shared func upload(b : Blob, name : Text, typ : Text) : async Text {
    let { fileId } = await D3.storeFile({
      d3;
      storeFileInput = { fileDataObject = b; fileName = name; fileType = typ };
    });
    fileId
  };

  public query func download(id : Text) : async ?Blob {
    switch (D3.getFile({ d3; getFileInput = { fileId = id } })) {
      case (?f) ?f.fileData;
      case null  null;
    }
  };
}
```

---

## ğŸ› ï¸Â Public API

| Operation                 | Description                               | Worstâ€‘case Time  | Notes                                                             |
| ------------------------- | ----------------------------------------- | ---------------- | ----------------------------------------------------------------- |
| `storeFile`               | Atomic singleâ€‘blob upload.                | **O(RÂ Â·Â logâ€¯F)** | R = storage regions, F = free blocks in scanned region.           |
| `storeFileMetadata`       | Reserve space, mark file **#Pending**.    | O(RÂ Â·Â logâ€¯F)     | Same allocator path without data write.                           |
| `storeFileChunk`          | Write chunk *k* (0â€‘based).                | **O(1)**         | Direct region write. Final chunk flip is a single `BTree.insert`. |
| `getFileMetadata`         | Return immutable header.                  | **O(logâ€¯N)**     | N = number of files (Bâ€‘Tree lookup).                              |
| `getFile`                 | Read entire file (â‰¤2Â MiB).                | **O(1Â +Â S)**     | S = size copied to return blob.                                   |
| `getFileHTTP`             | Streaming HTTP interface.                 | O(1) per chunk   | Uses callback token strategy.                                     |
| `getFileIds`              | List all files.                           | **O(N)**         | Linear scan over `fileLocationMap`.                               |
| `deleteFile`              | Deallocate + coalesce.                    | **O(logâ€¯F)**     | Two `BTree` lookups + optional scanLimit(1).                      |
| `cleanupAbandonedUploads` | Batch reclaim stale **#Pending** uploads. | **O(LÂ +Â logâ€¯N)** | L = `limit` parameter.                                            |

*Underlying primitives*

| Helper                          | Complexity                  | Rationale                                       |
| ------------------------------- | --------------------------- | ----------------------------------------------- |
| `Allocator.addToFreeLists`      | **O(logâ€¯F)**                | Two `BTree.insert` calls.                       |
| `Allocator.removeFromFreeLists` | **O(logâ€¯F)**                | DeleteÂ +Â vector swapâ€‘pop.                       |
| `Allocator._findAvailableSpace` | **O(RÂ Â·Â ( logâ€¯F + logâ€¯S))** | Single pass: bestâ€‘fit scan + append evaluation. |

---

## ğŸ—„ï¸Â Data Layout

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StorageRegion (4Â GiB max)                       â”‚
â”‚  â€¢ bumpâ€‘offset tail                             â”‚
â”‚  â€¢ BTree<Nat64,Nat64>  freeBlocksByOffset       â”‚
â”‚  â€¢ BTree<Nat64,Vec<Nat64>> freeListBySize       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ File Record (@ offset)                          â”‚
â”‚  00  Nat64  STORAGE_CLASS_CODE                  â”‚
â”‚  08  Nat64  fileSize                            â”‚
â”‚  10  Nat64  fileNameSize                        â”‚
â”‚  18  Nat64  fileTypeSize                        â”‚
â”‚  20  Blob   fileData                            â”‚
â”‚      Blob   fileName                            â”‚
â”‚      Blob   fileType                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

All integer fields are littleâ€‘endian **Nat64** and are 8â€‘byte aligned via `Utils.round_up_to_8`.

---

## ğŸ§ªÂ Testing

```bash
dfx deploy test
dfx canister call test run_all_tests
```

This executes:

* **Deterministic scenario** â€“ full lifecycle of upload â†’ HTTP download â†’ delete â†’ cleanup.
* **Propertyâ€‘based fuzz** â€“Â 200Ã— random allocations / frees with region invariants checked after **every** op.

---

## ğŸ“šÂ Reference Tables

### Enum Types

* `FileStatus = #Pending | #Complete`
* `StreamingStrategy = #Callback { callback; token }`

### Constants

| Symbol         | Value           | Meaning                                     |
| -------------- | --------------- | ------------------------------------------- |
| `PAGE_SIZE`    | 65â€¯536Â B        | IC stableâ€‘heap page.                        |
| `CHUNK_SIZE`   | 1â€¯800â€¯000Â B     | Fits comfortably under 2Â MiB message limit. |
| `BYTES_BUDGET` | 3â€¯758â€¯096â€¯384Â B | 3.5Â GiB safety ceiling.                     |

---

## ğŸ”’Â Safety & Invariants

* **Region accounting** â€“ For every region: `live + free + tail == capacity` (verified in tests).
* **Dualâ€‘map free lists** â€“ Offset â†”ï¸ size views kept in lockâ€‘step; violations trap.
* **Overflowâ€‘safe math** â€“ All `Nat64` additions use `Utils.checked_add`.

---

## ğŸ”—Â Integrating With HTTP

Expose two entrypoints on your actor:

```motoko
public query func http_request(req : HttpRequest) : async HttpResponse {
  D3.getFileHTTP({ d3; httpRequest = req; httpStreamingCallbackActor = this });
};

public shared query func http_request_streaming_callback(tok : StreamingCallbackToken)
  : async StreamingCallbackHttpResponse {
  D3.httpStreamingCallback({ d3; streamingCallbackToken = tok });
}
```

---

## ğŸ¤Â Contributing

Issues and PRs are welcome!  Please run `dfx test` and ensure *all* property tests pass before submitting.

---

## ğŸªªÂ License

MIT Â©Â 2025 Ztudio
